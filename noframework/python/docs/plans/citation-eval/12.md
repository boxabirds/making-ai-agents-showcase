# Task 12: Calibration Workflow

## Design Reference
- [tech-design.md §7.3](../../design/citation-eval/tech-design.md#73-calibration-metrics)
- [tech-design.md §7.4](../../design/citation-eval/tech-design.md#74-disagreement-analysis)

## Objective
Measure and improve LLM judge alignment with human judgments.

## Requirements

### R12.1: Agreement Calculation
Per tech-design.md §7.3:
- Compare LLM verdict to human score
- LLM supports ↔ human >= 2
- LLM not-supports ↔ human <= 1
- Calculate agreement rate

### R12.2: Correlation Metrics
- Spearman correlation between LLM confidence and human score
- Target: ρ > 0.6

### R12.3: Disagreement Analysis
Per tech-design.md §7.4:
- Categorize: false positive, false negative
- Log cases for review
- Generate report of disagreement patterns

### R12.4: Inter-Annotator Agreement
Measure consistency between multiple human annotators

## Dependencies
- Task 9: Aggregation (provides LLM verdicts)
- Task 11: Argilla setup (provides human annotations)

## Test Plan

| Test | Input | Expected |
|------|-------|----------|
| High agreement | LLM and humans match 90% | agreement_rate=0.9 |
| Low agreement | LLM and humans match 50% | agreement_rate=0.5, warning |
| Disagreement report | 10 disagreements | Categorized list |
| Correlation | Varied confidence/scores | Spearman ρ calculated |
| Multi-annotator | 3 annotators on same data | IAA metric reported |

## Acceptance Criteria
- [ ] Agreement rate calculation per tech-design.md §7.3
- [ ] Correlation computed and reported
- [ ] Disagreements categorized and logged
- [ ] Actionable recommendations for prompt tuning
- [ ] IAA metric for multi-annotator scenarios
