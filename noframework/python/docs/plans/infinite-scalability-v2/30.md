# Task 30: E2E: axios + architecture-overview-lite

## Objective
End-to-end integration test using axios repository and the reference prompt.

## Dependencies
- Task 29: Feature: citation verification
- Task 23: Remote repo cloning

## Deliverables

`tests/features/e2e.feature`:

```gherkin
Feature: End-to-end documentation generation
  As a user
  I want to generate documentation for a real repository
  So that I can verify the system works completely

  @slow @integration
  Scenario: Generate architecture overview for axios
    Given the prompt from "prompts/architecture-overview-lite.prompt.txt"
    And repository "https://github.com/axios/axios"
    When I run the full pipeline
    Then a report should be generated
    And the report should have multiple sections
    And all sections should have citations
    And at least 80% of citations should be valid
    And the report should include a mermaid diagram

  @slow @integration
  Scenario: Report covers key components
    Given the prompt from "prompts/architecture-overview-lite.prompt.txt"
    And repository "https://github.com/axios/axios"
    When I run the full pipeline
    Then the report should mention "Axios"
    And the report should mention "request"
    And the report should mention "interceptor"

  @slow @integration
  Scenario: Antipatterns identified
    Given the prompt from "prompts/architecture-overview-lite.prompt.txt"
    And repository "https://github.com/axios/axios"
    When I run the full pipeline
    Then the report should have a section about antipatterns or issues

  @slow @integration
  Scenario: CLI produces output file
    Given the prompt from "prompts/architecture-overview-lite.prompt.txt"
    And repository "https://github.com/axios/axios"
    When I run via CLI with output file "test-output.md"
    Then "test-output.md" should exist
    And "test-output.md" should contain valid markdown
```

## Step Definitions

`tests/step_defs/e2e_steps.py`:

```python
import subprocess
from pathlib import Path
from pytest_bdd import given, when, then, parsers
from tech_writer.cli import run as run_pipeline
from tech_writer.citations import verify_all_citations

@given(parsers.parse('the prompt from "{prompt_path}"'))
def load_prompt(prompt_path, context):
    context["prompt"] = Path(prompt_path).read_text()

@given(parsers.parse('repository "{repo_url}"'))
def set_repo(repo_url, context):
    context["repo"] = repo_url

@when("I run the full pipeline")
def run_full_pipeline(context, temp_dir):
    from tech_writer.orchestrator import run_pipeline
    context["report"], context["store"] = run_pipeline(
        prompt=context["prompt"],
        repo=context["repo"],
        cache_dir=temp_dir
    )

@when(parsers.parse('I run via CLI with output file "{output}"'))
def run_cli(output, context, temp_dir):
    result = subprocess.run([
        "python", "-m", "tech_writer",
        "--prompt", "prompts/architecture-overview-lite.prompt.txt",
        "--repo", context["repo"],
        "--output", str(temp_dir / output)
    ], capture_output=True, text=True)
    context["cli_result"] = result
    context["output_path"] = temp_dir / output

@then("a report should be generated")
def check_report_generated(context):
    assert context["report"] is not None
    assert len(context["report"]) > 500  # Reasonable length

@then("the report should have multiple sections")
def check_multiple_sections(context):
    # Count markdown headers
    headers = [l for l in context["report"].split('\n') if l.startswith('##')]
    assert len(headers) >= 3, f"Only {len(headers)} sections found"

@then("all sections should have citations")
def check_sections_have_citations(context):
    import re
    sections = context["report"].split('\n## ')[1:]  # Split by h2
    for section in sections:
        citations = re.findall(r'\[[^\]]+:\d+-\d+\]', section)
        assert len(citations) > 0, f"Section without citations: {section[:100]}"

@then(parsers.parse("at least {percent:d}% of citations should be valid"))
def check_citation_validity(percent, context):
    results, valid, invalid = verify_all_citations(
        context["report"],
        context["store"]
    )
    total = valid + invalid
    actual_percent = (valid / total * 100) if total > 0 else 0
    assert actual_percent >= percent, f"Only {actual_percent:.0f}% valid"

@then("the report should include a mermaid diagram")
def check_mermaid(context):
    assert "```mermaid" in context["report"]

@then(parsers.parse('the report should mention "{term}"'))
def check_mentions_term(term, context):
    assert term.lower() in context["report"].lower()

@then(parsers.parse('"{filename}" should exist'))
def check_file_exists(filename, context):
    assert context["output_path"].exists()
```

## Reference Prompt

From `prompts/architecture-overview-lite.prompt.txt`:
```
# Codebase Architectural Analysis: Lite version

**Objective:** Analyze the provided codebase from an architect's perspective
and generate a detailed architectural overview document. Focus on extracting
insights directly supported by the code, rather than providing exhaustive
file listings or making assumptions.

Describe the system in terms of microarchitectures and their relationships
in a mermaid diagram.

Call out potential antipatterns and design issues to watch out for.
```

## Success Criteria

| Metric | Threshold |
|--------|-----------|
| Report generated | Yes |
| Section count | >= 3 |
| Citation count | >= 10 |
| Valid citations | >= 80% |
| Contains mermaid | Yes |
| Mentions key terms | Axios, request, interceptor |
| Has antipatterns section | Yes |
| CLI produces file | Yes |

## Implementation Notes

- Mark tests with `@slow` and `@integration`
- Skip in CI fast runs: `pytest -m "not slow"`
- Cache axios clone between tests if possible
- Test may take 2-5 minutes with real LLM
- Consider mock LLM for faster CI tests
