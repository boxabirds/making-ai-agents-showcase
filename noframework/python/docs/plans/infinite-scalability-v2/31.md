# Task 31: Make Section Generation Agentic

## Objective
Fix critical gap: enable LLM to call tools during section generation, matching the design spec.

## Dependencies
- Task 18: Section generation (current non-agentic implementation)

## Problem Statement

The design spec (Phase 3) states:
> Section generation is itself agentic:
> - LLM can do more exploration if needed
> - Generate section content with citations

Current implementation in `orchestrator.py:283-337` makes a single `chat()` call:
```python
response = self.llm_client.chat(messages)
return response["content"] or ""
```

This prevents the LLM from exploring further when pre-gathered context is insufficient.

## Deliverables

`tech_writer/orchestrator.py` (modify `SectionGenerator`):

```python
SECTION_TOOLS = ["read_file", "get_symbols", "get_structure", "get_imports", "search_text"]

class SectionGenerator:
    def __init__(
        self,
        store: CacheStore,
        repo_root: Path,
        llm_client: LLMClient,
        max_exploration_steps: int = 5,
    ):
        self.store = store
        self.repo_root = repo_root
        self.llm_client = llm_client
        self.max_exploration_steps = max_exploration_steps

    def generate(
        self,
        section: Section,
        prompt: str,
        previous_sections: list[str],
    ) -> str:
        """Generate content for a section using agentic exploration."""
        # Build context from relevant files
        context = self._build_context(section)
        prev_summary = self._build_previous_summary(previous_sections)

        # Create tool handlers (same pattern as explore_codebase)
        tool_handlers = {
            "read_file": partial(read_file, store=self.store, repo_root=self.repo_root),
            "get_symbols": partial(get_symbols, store=self.store, repo_root=self.repo_root),
            "get_structure": partial(get_structure, store=self.store, repo_root=self.repo_root),
            "get_imports": partial(get_imports, store=self.store, repo_root=self.repo_root),
            "search_text": partial(search_text, store=self.store),
        }

        # Get tool definitions filtered to section-relevant tools
        tools = [t for t in get_tool_definitions() if t["function"]["name"] in SECTION_TOOLS]

        messages = [
            {"role": "system", "content": SECTION_SYSTEM_PROMPT},
            {"role": "user", "content": self._build_user_prompt(section, prompt, context, prev_summary)},
        ]

        # Run agentic loop with limited steps
        content, _ = self.llm_client.run_tool_loop(
            messages=messages,
            tools=tools,
            tool_handlers=tool_handlers,
            max_steps=self.max_exploration_steps,
        )

        return content or ""
```

Update `SECTION_SYSTEM_PROMPT`:

```python
SECTION_SYSTEM_PROMPT = """You are a technical documentation expert writing a section.

Write the content for this section based on the provided context.

You have tools available if you need more information:
- read_file(path): Read a file's contents
- get_symbols(path, kind): Get functions/classes in a file
- get_structure(path): Get structural overview of a file
- get_imports(path): Get imports in a file
- search_text(query): Search for text in cached files

Rules:
1. CITE YOUR SOURCES: Every claim must include a citation [path:start_line-end_line]
   Example: "The Axios class handles HTTP requests [lib/core/Axios.js:10-25]."

2. Be specific and accurate - only describe what you can see in the code

3. Use markdown formatting with appropriate headers

4. Don't repeat information from previous sections

5. Focus on the section's specific topic

6. If pre-gathered context is insufficient, use tools to read more files

7. When you have enough information, output the section content and stop calling tools
"""
```

## Acceptance Criteria

- [ ] `SectionGenerator.generate()` uses `run_tool_loop()` instead of `chat()`
- [ ] LLM can call `read_file`, `get_symbols`, `get_structure`, `get_imports`, `search_text`
- [ ] Tool results are added to the message history
- [ ] Max exploration steps is respected (default: 5)
- [ ] Files read during section generation are cached
- [ ] Final output is extracted from last assistant message
- [ ] Section generation still works when no tools are called

## Test Cases

```gherkin
Feature: Agentic section generation

  Scenario: Section generated without tool calls
    Given a section with relevant files already read
    And the LLM has sufficient context
    When I generate the section
    Then the section should be generated in one call
    And content should have citations

  Scenario: Section needs additional exploration
    Given a section titled "Database Layer"
    And relevant_files only includes README.md
    When I generate the section
    Then the LLM should call read_file for database files
    And the new files should be cached
    And the final content should cite the new files

  Scenario: Max exploration steps respected
    Given a section that could explore indefinitely
    And max_exploration_steps is 3
    When I generate the section
    Then at most 3 tool calls should be made
    And the section should still be generated

  Scenario: Tools return useful information
    Given a section about "API Endpoints"
    When the LLM calls get_structure("routes/api.py")
    Then it should receive structured info about functions
    And it can cite specific line ranges
```

## Implementation Notes

- Reuse existing `run_tool_loop()` from `LLMClient`
- Filter `get_tool_definitions()` to only include section-relevant tools
- `finish_exploration` is NOT included - section ends when LLM stops calling tools
- Extract final content from the last assistant message that doesn't have tool_calls
- Keep `_build_context()` and `_build_previous_summary()` as private methods
