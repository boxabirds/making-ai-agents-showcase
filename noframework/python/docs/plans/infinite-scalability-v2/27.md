# Task 27: Feature: Outline Generation

## Objective
BDD tests for outline generation from exploration results.

## Dependencies
- Task 26: Feature: semantic queries

## Deliverables

`tests/features/outline.feature`:

```gherkin
Feature: Outline generation
  As a tech writer agent
  I need to generate report outlines
  So that I can structure my documentation

  Background:
    Given the axios repository is available
    And exploration has completed with understanding:
      """
      Axios is an HTTP client library. Main components:
      - Core Axios class in lib/core/Axios.js
      - Request/response interceptors
      - Adapters for different environments
      """
    And the following files were explored:
      | path                          |
      | lib/axios.js                  |
      | lib/core/Axios.js             |
      | lib/core/dispatchRequest.js   |
      | README.md                     |

  Scenario: Generate outline
    Given prompt "Write an architectural overview of axios"
    When I generate an outline
    Then I should get a list of sections
    And each section should have title, focus, relevant_files

  Scenario: Reasonable section count
    Given prompt "Write documentation for axios"
    When I generate an outline
    Then section count should be between 3 and 15

  Scenario: Sections reference explored files
    Given prompt "Document the core functionality"
    When I generate an outline
    Then relevant_files should only contain explored files

  Scenario: Outline matches prompt scope
    Given prompt "Explain error handling in axios"
    When I generate an outline
    Then at least one section should relate to errors

  Scenario: JSON parsing
    Given prompt "Write documentation"
    When I generate an outline
    Then the outline should be valid JSON
    And parsing should not raise errors

  Scenario: Empty exploration
    Given no files were explored
    When I try to generate an outline
    Then an appropriate error should be raised
```

## Step Definitions

`tests/step_defs/outline_steps.py`:

```python
from pytest_bdd import given, when, then, parsers
from tech_writer.orchestrator import generate_outline, Section

@given(parsers.parse('exploration has completed with understanding:\n{understanding}'))
def set_understanding(understanding, context):
    context["understanding"] = understanding

@given("the following files were explored:")
def set_explored_files(datatable, context):
    context["cached_files"] = [row["path"] for row in datatable]

@given("no files were explored")
def set_no_files(context):
    context["cached_files"] = []

@given(parsers.parse('prompt "{prompt}"'))
def set_prompt(prompt, context):
    context["prompt"] = prompt

@when("I generate an outline")
def generate_outline_step(context, mock_llm):
    context["outline"] = generate_outline(
        prompt=context["prompt"],
        understanding=context["understanding"],
        cached_files=context["cached_files"],
        llm_client=mock_llm
    )

@then("I should get a list of sections")
def check_sections_list(context):
    assert isinstance(context["outline"], list)
    assert len(context["outline"]) > 0

@then("each section should have title, focus, relevant_files")
def check_section_fields(context):
    for section in context["outline"]:
        assert "title" in section or hasattr(section, "title")
        assert "focus" in section or hasattr(section, "focus")
        assert "relevant_files" in section or hasattr(section, "relevant_files")

@then(parsers.parse("section count should be between {min:d} and {max:d}"))
def check_section_count(min, max, context):
    count = len(context["outline"])
    assert min <= count <= max, f"Got {count} sections"

@then("relevant_files should only contain explored files")
def check_relevant_files(context):
    explored = set(context["cached_files"])
    for section in context["outline"]:
        files = section.relevant_files if hasattr(section, "relevant_files") else section["relevant_files"]
        for f in files:
            assert f in explored, f"{f} was not explored"
```

## Mock LLM Fixture

```python
@pytest.fixture
def mock_llm():
    """Mock LLM that returns predefined outline."""
    class MockLLM:
        def chat_completions_create(self, **kwargs):
            # Return a valid outline JSON
            return MockResponse('''[
                {"title": "Overview", "focus": "Introduction", "relevant_files": ["README.md"]},
                {"title": "Core", "focus": "Main class", "relevant_files": ["lib/core/Axios.js"]}
            ]''')
    return MockLLM()
```

## Implementation Notes

- Use mock LLM for deterministic tests
- Real LLM tests are integration tests (separate)
- Validate JSON parsing and Section creation
- Test edge cases: empty files, invalid JSON
