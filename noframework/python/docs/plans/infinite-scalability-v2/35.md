# Task 35: E2E Integration Test

## Objective
Create end-to-end integration tests that verify the full pipeline against fixture repositories.

## Dependencies
- Task 30: E2E axios test (marked done, but no test file exists)
- Task 31: Agentic section generation
- Task 34: Citation re-generation

## Problem Statement

Design spec (lines 395-398) requires:
> Testing Strategy:
> 4. Integration tests: End-to-end on small fixture repos

No E2E test files exist. The existing tests are unit tests with mock fixtures.

## Deliverables

### 1. Create fixture repository

`tests/fixtures/sample_flask_app/` - A minimal Flask app for testing:

```
tests/fixtures/sample_flask_app/
├── README.md
├── app.py
├── routes/
│   ├── __init__.py
│   ├── users.py
│   └── health.py
├── models/
│   ├── __init__.py
│   └── user.py
└── requirements.txt
```

### 2. Create E2E test file

`tests/tech_writer/test_e2e.py`:

```python
"""
End-to-end integration tests for tech_writer.

These tests run the full pipeline against a fixture repository.
They require OPENAI_API_KEY or a mock LLM.
"""

import os
import pytest
from pathlib import Path

from tech_writer.orchestrator import run_pipeline
from tech_writer.citations import verify_all_citations, extract_citations


FIXTURE_REPO = Path(__file__).parent.parent / "fixtures" / "sample_flask_app"
SKIP_E2E = os.environ.get("SKIP_E2E_TESTS", "").lower() in ("1", "true", "yes")


@pytest.fixture
def fixture_repo():
    """Path to the fixture Flask app."""
    assert FIXTURE_REPO.exists(), f"Fixture repo not found: {FIXTURE_REPO}"
    return FIXTURE_REPO


@pytest.mark.skipif(SKIP_E2E, reason="E2E tests skipped via SKIP_E2E_TESTS env var")
class TestE2EPipeline:
    """End-to-end pipeline tests."""

    def test_full_pipeline_produces_report(self, fixture_repo):
        """Test that the pipeline produces a non-empty report."""
        prompt = "Document the API endpoints in this Flask application."

        report, store = run_pipeline(
            prompt=prompt,
            repo=str(fixture_repo),
            max_exploration=20,
            max_sections=5,
        )

        assert report, "Report should not be empty"
        assert "API" in report or "endpoint" in report.lower()
        assert store.list_cached_files(), "Some files should be cached"

    def test_report_has_citations(self, fixture_repo):
        """Test that the report contains citations."""
        prompt = "Explain how user authentication works."

        report, store = run_pipeline(
            prompt=prompt,
            repo=str(fixture_repo),
            max_exploration=15,
            max_sections=3,
        )

        citations = extract_citations(report)
        assert len(citations) > 0, "Report should contain citations"

    def test_citations_are_valid(self, fixture_repo):
        """Test that citations reference cached files with valid line ranges."""
        prompt = "Document the data models."

        report, store = run_pipeline(
            prompt=prompt,
            repo=str(fixture_repo),
            max_exploration=15,
            max_sections=3,
        )

        results, valid, invalid = verify_all_citations(report, store)

        # Allow some invalid citations but majority should be valid
        if valid + invalid > 0:
            validity_rate = valid / (valid + invalid)
            assert validity_rate >= 0.5, f"Citation validity too low: {validity_rate:.0%}"

    def test_exploration_respects_max_steps(self, fixture_repo):
        """Test that exploration stops at max_exploration."""
        prompt = "Document everything in extreme detail."

        report, store = run_pipeline(
            prompt=prompt,
            repo=str(fixture_repo),
            max_exploration=5,  # Very low limit
            max_sections=2,
        )

        # With only 5 exploration steps, we shouldn't read too many files
        cached_files = store.list_cached_files()
        assert len(cached_files) <= 10, f"Too many files cached: {len(cached_files)}"

    def test_max_sections_respected(self, fixture_repo):
        """Test that outline is truncated to max_sections."""
        prompt = "Create comprehensive documentation covering every aspect."

        report, store = run_pipeline(
            prompt=prompt,
            repo=str(fixture_repo),
            max_exploration=10,
            max_sections=2,
        )

        # Count top-level sections (## headers)
        section_count = report.count("\n## ")
        assert section_count <= 2, f"Too many sections: {section_count}"


@pytest.mark.skipif(SKIP_E2E, reason="E2E tests skipped")
class TestE2ECLIIntegration:
    """Test CLI integration."""

    def test_cli_produces_output(self, fixture_repo, tmp_path):
        """Test that CLI produces output file."""
        import subprocess

        prompt_file = tmp_path / "prompt.md"
        prompt_file.write_text("List the main components of this Flask app.")

        output_file = tmp_path / "report.md"

        result = subprocess.run(
            [
                "python", "-m", "tech_writer",
                "--prompt", str(prompt_file),
                "--repo", str(fixture_repo),
                "--output", str(output_file),
                "--max-exploration", "10",
                "--max-sections", "3",
            ],
            capture_output=True,
            text=True,
            timeout=120,
        )

        assert result.returncode == 0, f"CLI failed: {result.stderr}"
        assert output_file.exists(), "Output file should be created"
        assert output_file.read_text(), "Output file should not be empty"

    def test_cli_verify_citations(self, fixture_repo, tmp_path):
        """Test that --verify-citations flag works."""
        import subprocess

        prompt_file = tmp_path / "prompt.md"
        prompt_file.write_text("Document the routes.")

        result = subprocess.run(
            [
                "python", "-m", "tech_writer",
                "--prompt", str(prompt_file),
                "--repo", str(fixture_repo),
                "--verify-citations",
                "--max-exploration", "10",
                "--max-sections", "2",
            ],
            capture_output=True,
            text=True,
            timeout=120,
        )

        assert result.returncode == 0
        # Citation stats should be in stderr
        assert "Citation" in result.stderr or "citation" in result.stderr
```

### 3. Create fixture Flask app files

`tests/fixtures/sample_flask_app/README.md`:
```markdown
# Sample Flask App

A minimal Flask application for testing tech_writer.

## Features
- User management endpoints
- Health check endpoint
- SQLAlchemy models
```

`tests/fixtures/sample_flask_app/app.py`:
```python
"""Main Flask application."""

from flask import Flask
from routes import users, health

def create_app():
    """Create and configure the Flask app."""
    app = Flask(__name__)
    app.config["SECRET_KEY"] = "dev"

    # Register blueprints
    app.register_blueprint(users.bp)
    app.register_blueprint(health.bp)

    return app

if __name__ == "__main__":
    app = create_app()
    app.run(debug=True)
```

`tests/fixtures/sample_flask_app/routes/users.py`:
```python
"""User management routes."""

from flask import Blueprint, jsonify, request

bp = Blueprint("users", __name__, url_prefix="/users")

@bp.route("/", methods=["GET"])
def list_users():
    """List all users."""
    return jsonify({"users": []})

@bp.route("/<int:user_id>", methods=["GET"])
def get_user(user_id):
    """Get a specific user by ID."""
    return jsonify({"id": user_id, "name": "Test User"})

@bp.route("/", methods=["POST"])
def create_user():
    """Create a new user."""
    data = request.get_json()
    return jsonify({"id": 1, "name": data.get("name")}), 201
```

`tests/fixtures/sample_flask_app/routes/health.py`:
```python
"""Health check routes."""

from flask import Blueprint, jsonify

bp = Blueprint("health", __name__)

@bp.route("/health", methods=["GET"])
def health_check():
    """Return application health status."""
    return jsonify({"status": "healthy"})
```

`tests/fixtures/sample_flask_app/models/user.py`:
```python
"""User model."""

from dataclasses import dataclass
from datetime import datetime

@dataclass
class User:
    """Represents a user in the system."""
    id: int
    name: str
    email: str
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()
```

## Acceptance Criteria

- [ ] Fixture Flask app exists with realistic structure
- [ ] E2E tests run full pipeline (exploration → outline → sections → assembly)
- [ ] Tests verify report is non-empty
- [ ] Tests verify citations exist
- [ ] Tests verify citation validity rate > 50%
- [ ] Tests verify `max_exploration` is respected
- [ ] Tests verify `max_sections` is respected
- [ ] CLI integration test verifies end-to-end flow
- [ ] Tests can be skipped via `SKIP_E2E_TESTS=1` for CI without API keys

## Test Cases

```gherkin
Feature: E2E pipeline

  Scenario: Generate report for Flask app
    Given the sample_flask_app fixture
    And a prompt "Document the API endpoints"
    When I run the full pipeline
    Then a report should be generated
    And the report should mention "users" or "health"
    And the report should have citations

  Scenario: Citations reference real files
    Given the sample_flask_app fixture
    When I run the pipeline
    Then citations should reference files that exist
    And line numbers should be within file bounds

  Scenario: Exploration limits work
    Given max_exploration=5
    When I run the pipeline
    Then at most ~10 files should be cached
```

## Implementation Notes

- Use a small fixture repo (not axios) for fast, deterministic tests
- Tests require `OPENAI_API_KEY` or equivalent
- Add `SKIP_E2E_TESTS=1` env var for CI without API access
- Consider adding mock LLM option for offline testing
- Test timeout should be generous (120s) for API latency
- Fixture repo should be small enough for quick exploration
