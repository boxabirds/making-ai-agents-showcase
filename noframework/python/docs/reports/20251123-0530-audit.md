# Audit: tech_writer v2 (infinite-scalability-v2)

**Date:** 2025-11-23
**Auditor:** Claude Sonnet 4.5
**Verdict:** Implementation is functionally complete. Remaining gaps are operational polish, not architectural deficiencies.

---

## Executive Summary

The v2 redesign successfully addressed the catastrophic O(chunks) LLM calls problem identified in the previous audit. The implementation now follows the spec:

- **Agentic exploration**: LLM decides what to read via semantic tools
- **Lazy caching**: Only explored files are cached
- **Section-by-section output**: Each section generated independently
- **Citation grounding**: All claims verified against cached content

The 35 tasks in `docs/plans/infinite-scalability-v2/tasks.md` are marked complete. This audit identifies residual gaps not covered by those tasks.

---

## Design Compliance Matrix

| Spec Requirement | Status | Notes |
|-----------------|--------|-------|
| Agentic exploration (Phase 1) | ✅ | `orchestrator.py:260-304` |
| 9 semantic tools | ✅ | All implemented in `tools/` |
| Outline generation (Phase 2) | ✅ | `orchestrator.py:307-367` |
| Agentic section generation (Phase 3) | ✅ | `SectionGenerator` class |
| Citation verification (Phase 4) | ✅ | `citations.py` + re-generation |
| SQLite + FTS5 | ✅ | `store.py` with BM25 scoring |
| Tree-sitter parsing | ✅ | 14 languages in `parser.py` |
| CLI with all flags | ✅ | `cli.py` |
| Remote repo cloning | ✅ | `repo.py` |
| E2E tests | ✅ | `test_e2e.py` |

---

## Identified Gaps

### P1: Critical

*None.* Core functionality complete.

### P2: High

#### GAP-01: Task 30 "E2E: axios + architecture-overview-lite" is marked partial

**Location:** `tasks.md:39`

The tasks list shows Task 30 as `⚠️ partial`. There's no actual test running against the axios repo with the reference prompt. The E2E tests (`test_e2e.py`) use a synthetic `sample_flask_app` fixture, not a real-world codebase.

**Impact:** No validation that the system works on large, complex repos.

**Proposed task:** Create integration test that clones axios (or similar mid-size OSS repo) and runs with a reference prompt, asserting:
- Report generated
- Citation validity rate > 80%
- Exploration stays within step limits

---

#### GAP-02: No token/cost tracking

**Location:** Design spec line 4: "Cost scales with query complexity, not repo size"

The claim is architecturally sound (only explored files are processed), but there's no instrumentation to verify or report this. No way to know:
- How many tokens were used per phase
- LLM calls per exploration step
- Total cost of a run

**Impact:** Can't validate cost claims or optimize.

**Proposed task:** Add optional `--verbose-stats` flag that outputs:
- Files read, symbols extracted
- LLM calls per phase (exploration, outline, section generation)
- Token counts (via OpenAI response `usage` field)

---

### P3: Medium

#### GAP-03: No retry logic for LLM API calls

**Location:** `llm.py:236-270` (`chat` method)

API calls have no retry on transient failures (rate limits, network errors). A single 429 or timeout crashes the entire run.

**Impact:** Unreliable for long-running reports.

**Proposed task:** Add exponential backoff retry (3 attempts) for recoverable errors in `LLMClient.chat()`.

---

#### GAP-04: No streaming support

**Location:** `llm.py`

All LLM calls use `stream=False`. For long section generation, user sees no progress for potentially minutes.

**Impact:** Poor UX for interactive use.

**Proposed task:** Add `--stream` CLI flag that enables streaming, printing tokens as they arrive during section generation.

---

#### GAP-05: Serial section generation

**Location:** `orchestrator.py:188-195`

Sections are generated sequentially:
```python
for section in outline:
    content = generator.generate(...)
```

Sections don't depend on each other's *content* (only headers via `previous_sections`), so this could be parallelized.

**Impact:** Slow for multi-section reports.

**Proposed task:** Add `--parallel-sections` flag that generates sections concurrently (respecting `previous_sections` header summarization).

---

#### GAP-06: pyproject.toml metadata incorrect

**Location:** `pyproject.toml:2-4`

```toml
name = "baseline"
version = "0.1.0"
description = "Add your description here"
```

Package name should be `tech_writer`, description should reflect the tool's purpose.

**Impact:** Confusing if installed, can't be published to PyPI.

**Proposed task:** Update pyproject.toml with correct metadata.

---

#### GAP-07: Missing pytest in dev dependencies

**Location:** `pyproject.toml`

pytest isn't listed in dependencies. Tests rely on pytest but it's not declared.

**Impact:** `uv run pytest` works because pytest is installed separately, but clean installs would fail.

**Proposed task:** Add `[project.optional-dependencies]` with dev/test deps: pytest, pytest-cov.

---

### P4: Low

#### GAP-08: Uneven language support testing

**Location:** `parser.py`

14 languages are listed as supported, but only Python and JavaScript have symbol/import extraction tests. Go, Rust, Java, etc. are implemented but untested.

**Impact:** Potential bugs in less-common language parsing.

**Proposed task:** Add `test_parser_languages.py` with extraction tests for Go, Rust, Java, TypeScript.

---

#### GAP-09: No progress callbacks during exploration

**Location:** `orchestrator.py:260-304`

During exploration, there's no way to know what the LLM is doing. For repos requiring many exploration steps, user sees nothing until completion.

**Impact:** Poor UX, can't tell if run is stuck.

**Proposed task:** Add optional progress callback to `run_pipeline()` that fires on each tool call with tool name and args.

---

#### GAP-10: `__init__.py` exports incomplete

**Location:** `tech_writer/__init__.py`

Module exports are minimal. Public API isn't clearly defined.

**Impact:** Users importing the library don't know what's public.

**Proposed task:** Add `__all__` with public API: `run_pipeline`, `CacheStore`, `verify_all_citations`.

---

#### GAP-11: Test fixture diversity

**Location:** `tests/fixtures/`

Only one fixture repo: `sample_flask_app` (Python/Flask). No multi-language repos, no TypeScript, no Go.

**Impact:** Can't verify multi-language exploration works E2E.

**Proposed task:** Add `sample_ts_project` fixture with TypeScript + tests, for multi-language test coverage.

---

#### GAP-12: No dry-run mode

**Location:** CLI

No way to preview what exploration would do without making LLM API calls.

**Impact:** Can't estimate cost before running.

**Proposed task:** Add `--dry-run` flag that runs exploration with mock LLM, outputting tool calls that would be made.

---

## Test Coverage Assessment

| Component | Unit Tests | E2E Tests | Coverage |
|-----------|-----------|-----------|----------|
| CacheStore | 6 tests | - | Good |
| list_files | 5 tests | - | Good |
| read_file | 5 tests | - | Good |
| Parser (Python) | 3 tests | - | Good |
| Parser (JS) | 2 tests | - | Good |
| Parser (Go/Rust/etc.) | 0 tests | - | **Gap** |
| Semantic tools | 6 tests | - | Good |
| Citations | 9 tests | - | Good |
| Full pipeline | - | 5 tests | Good |
| Real-world repo | - | 0 tests | **Gap** |

---

## Proposed Tasks Summary

| ID | Task | Priority | Effort |
|----|------|----------|--------|
| GAP-01 | Real-world repo E2E test (axios) | P2 | Medium |
| GAP-02 | Token/cost tracking (`--verbose-stats`) | P2 | Medium |
| GAP-03 | LLM API retry logic | P3 | Small |
| GAP-04 | Streaming support (`--stream`) | P3 | Medium |
| GAP-05 | Parallel section generation | P3 | Medium |
| GAP-06 | Fix pyproject.toml metadata | P3 | Trivial |
| GAP-07 | Add pytest to dev dependencies | P3 | Trivial |
| GAP-08 | Multi-language parser tests | P4 | Small |
| GAP-09 | Progress callbacks | P4 | Small |
| GAP-10 | Module exports cleanup | P4 | Trivial |
| GAP-11 | TypeScript test fixture | P4 | Small |
| GAP-12 | Dry-run mode | P4 | Medium |

---

## Recommended Execution Order

```
1. GAP-06, GAP-07, GAP-10  (Trivial fixes, do together)
2. GAP-03                   (Retry logic - reliability)
3. GAP-02                   (Stats - visibility into what's happening)
4. GAP-01                   (Real-world validation)
5. GAP-08, GAP-11           (Test coverage expansion)
6. GAP-09                   (Progress UX)
7. GAP-04, GAP-05, GAP-12   (Performance/UX polish)
```

---

## Conclusion

The tech_writer v2 implementation successfully delivers on the design spec. The agentic exploration model solves the O(chunks) LLM call problem from v1. All core functionality is implemented and tested.

Remaining gaps are operational: reliability (retry logic), observability (stats), and polish (streaming, parallelism). None are architectural blockers.

**Recommendation:** Ship it. Address P2 gaps before production use on large repos.
