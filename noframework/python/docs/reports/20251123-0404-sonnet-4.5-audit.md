# LLM Efficiency Audit: infinite-scalability Branch

**Date:** 2025-11-23
**Auditor:** Claude Sonnet 4.5
**Verdict:** The implementation is fundamentally broken. It scales *worse* than a naive single-LLM-call approach would for arbitrary-size codebases.

---

## Executive Summary

The "infinite-scalability" design aspires to handle arbitrary-size codebases without context overruns. The implementation achieves the opposite: **LLM calls scale linearly with chunks**, making large repos catastrophically expensive. A 100-file repo with ~10 chunks per file triggers **1,100+ LLM calls** in the summarization phase alone—before any report drafting or verification.

The core problem: the implementation conflates "avoiding context overruns" with "summarize everything eagerly." These are different problems requiring different solutions.

---

## Critical Issues

### 1. Summarization Is O(chunks) LLM Calls (Catastrophic)

**Location:** `summarize.py:53-67` (`summarize_project`)

```python
for file_rec in store.get_all_files():
    chunk_summaries.extend(summarize_chunks(store, file_rec.id))  # LLM per chunk
    file_summaries.append(summarize_file(store, file_rec.id))    # LLM per file
```

For each file, the code:
1. Calls LLM for **every chunk** (`summarize_chunks` → `SummarizeFileModule` per chunk)
2. Calls LLM **again** for the whole file (`summarize_file` → `SummarizeFileModule`)

**Cost model:**
- F files × C average chunks per file = F×C + F = **F×(C+1) LLM calls**
- 100 files × 10 chunks = **1,100 LLM calls** just for summarization
- 1,000 files × 10 chunks = **11,000 LLM calls**

**The fatal flaw:** These summaries are stored but *barely used*. The retrieval path (`retrieve_context`) primarily uses FTS on raw chunks, not summaries. The summaries are dead weight.

### 2. DSPy Is Cargo-Culted (Design Fraud)

**Location:** `dspy_pipeline.py`

The design document promises:
> "DSPy modules encode pipeline stages and tool usage; DSPy compiler can tune prompts/policies."

Reality:
```python
class SummarizeFileModule(dspy.Module):
    def forward(self, file_path: str, content: str):
        summary = summarize_text(content, ...)  # Just wraps OpenAI call
        return {"text": summary.text, ...}

class SummarizeModuleModule(dspy.Module):
    def forward(self, module_path: str, child_summaries: list[str]):
        text = f"Module: {module_path}\n" + "\n".join(child_summaries)  # No LLM at all!
        return {"text": text, ...}
```

- `SummarizeFileModule`: A hollow wrapper around `summarize_text()` with no DSPy signatures, no optimization
- `SummarizeModuleModule`: **Doesn't even call an LLM**—just string concatenation pretending to be "reduce"
- No DSPy signatures (`dspy.Signature`)
- No DSPy compiler usage
- No prompt optimization
- No few-shot examples or demonstrations

**This is not using DSPy. It's importing DSPy and doing nothing with it.**

### 3. Claim Checking Is O(claims × chunks) LLM Calls

**Location:** `claims.py:93-171` (`check_claims`)

For each claim:
1. Grade each existing citation against evidence → **LLM call per citation**
2. If no support found, retrieve 5 chunks and grade each → **up to 5 more LLM calls**

```python
for claim in claims:
    for cit in claim.citation_refs:
        graded = _grade_claim(claim.text, chunk.text, grader)  # LLM call
    if not supported and not contradicted:
        ctx = retrieve_context(store, claim.text, limit=5)
        for c in ctx.chunks:
            graded = _grade_claim(claim.text, c.text, grader)  # LLM call
```

**Cost model:**
- M claims × (existing citations + 5 fallback chunks) = worst case **M × 6 LLM calls per iteration**
- 20 claims × 3 iterations = **360 LLM calls** worst case for claim checking

### 4. Per-Line Citation Enforcement

**Location:** `enforcement.py:9-80` (`enforce_draft_citations`)

Every line without a citation triggers a retrieval call:
```python
for line in lines:
    if not tokens:
        ctx = retrieve_context(store, stripped or topic, limit=5)  # per line!
```

While `retrieve_context` isn't an LLM call, this is still O(lines) retrieval operations, defeating the purpose of batch processing.

### 5. Embeddings Are Phantom

**Location:** `retrieval.py:106-146` (`retrieve_embeddings`)

The design promises "optional embeddings" as a recall booster. Reality:
- `ingest.py` generates embeddings from **SHA256 hashes**, not semantic vectors
- No actual embedding model is called
- The "embeddings" are deterministic hashes masquerading as vectors

```python
h = hashlib.sha256(chunk_text.encode("utf-8")).digest()
vec = [int.from_bytes(seg, "little") / 2**32 for i in range(8)]  # Not embeddings!
```

---

## Cost Analysis: 100-File Repo

| Phase | LLM Calls | Notes |
|-------|-----------|-------|
| Summarize chunks | 1,000 | 100 files × 10 chunks |
| Summarize files | 100 | 1 per file |
| Draft report | 1 | Initial draft |
| Check claims (iter 1) | 120 | 20 claims × 6 worst case |
| Repair/revise (iter 1) | 1 | Re-draft |
| Check claims (iter 2) | 120 | |
| Repair/revise (iter 2) | 1 | |
| Check claims (iter 3) | 120 | |
| **Total** | **~1,463** | Conservative estimate |

For comparison, a naive approach that just concatenates files into chunks up to context limit and makes 1 LLM call would cost... **1 LLM call**.

---

## Why The Design Is Incoherent

The design document states:
> "Handle arbitrary-size codebases without context overruns."

The implemented solution interprets this as "summarize every chunk separately to avoid context limits." But this creates a worse problem:
- **Token cost scales with repo size** regardless of what the user actually asked
- **Latency scales with repo size** (1,100 serial LLM calls for summarization)
- **The summaries aren't used** for the actual report generation

The correct interpretation is: **retrieve only what's needed for the specific query**. This is what RAG systems do. The implementation does the opposite: it processes everything upfront, stores summaries that aren't used, then does retrieval anyway.

---

## Recommendations

### Immediate (Stop the Bleeding)

1. **Delete summarization entirely.** It's pure waste—1,000+ LLM calls that produce unused artifacts.

2. **Remove DSPy.** It's not being used. Either delete it or actually implement proper signatures and compilers.

3. **Remove embeddings.** SHA256 hashes are not embeddings. Either implement real embeddings or delete the code.

4. **Batch claim checking.** Instead of 1 LLM call per claim-evidence pair, send all claims + evidence in one prompt.

### Architectural (Fix The Design)

1. **Invert the pipeline:** Don't summarize eagerly. Instead:
   ```
   Query → Retrieve relevant chunks → Draft with citations → Verify
   ```

2. **Use the context window:** GPT-4o-mini has 128K context. For most repos, you can fit substantial code directly without summarization.

3. **Iterative deepening:** Start with coarse retrieval, refine if needed. Don't front-load all processing.

---

## Conclusion

The "infinite-scalability" branch has a fundamental architectural flaw: it conflates "handling large repos" with "processing everything eagerly." The result is a system that costs O(chunks) LLM calls regardless of query complexity.

The path forward is to invert the architecture: lazy/on-demand processing, query-driven retrieval, and batched LLM calls.

---

## Appendix: File-by-File LLM Call Sources

| File | Function | LLM Calls | Notes |
|------|----------|-----------|-------|
| `llm.py` | `summarize_text()` | 1 per call | Core LLM wrapper |
| `llm.py` | `draft_report()` | 1 per call | Report generation |
| `summarize.py` | `summarize_chunks()` | N per file | N = chunk count |
| `summarize.py` | `summarize_file()` | 1 per file | |
| `summarize.py` | `summarize_module()` | 0 | No LLM! Just concat |
| `summarize.py` | `summarize_package()` | 0 | No LLM! Just concat |
| `claims.py` | `_grade_claim()` | 1 per claim-evidence | Via `summarize_text` |
| `claims.py` | `check_claims()` | ≤6 per claim | Calls `_grade_claim` |
